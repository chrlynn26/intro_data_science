{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Working with Text\n",
    "\n",
    "In this assignment, I worked with messy medical data and used regex to extract relevant infromation from the data. The goal of this assignment was to correctly identify and sort all of the date variants encoded in this dataset. Once I extracted all the date patterns from the text, I then normalized the data and sorted them in ascending chronological order, according to the following given rules: \n",
    "\n",
    "* Assume all dates in xx/xx/xx format are mm/dd/yy\n",
    "* Assume all dates where year is encoded in only two digits are years from the 1900's (e.g. 1/5/89 is January 5th, 1989)\n",
    "* If the day is missing (e.g. 9/2009), assume it is the first day of the month (e.g. September 1, 2009).\n",
    "* If the month is missing (e.g. 2010), assume it is the first of January of that year (e.g. January 1, 2010).\n",
    "* Watch out for potential typos as this is a raw, real-life derived dataset.\n",
    "\n",
    "After sorting the medical data using the rules above, I was instructed to return the correct date from each medical note and return a pandas Series in chronological order of the original Series' indices.\n",
    "\n",
    "*Note: Each line of the `dates.txt` file corresponds to a medical note. Each note has a date that needs to be extracted, but each date is encoded in one of many formats.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         03/25/93 Total time of visit (in minutes):\\n\n",
       "1                       6/18/85 Primary Care Doctor:\\n\n",
       "2    sshe plans to move as of 7/8/71 In-Home Servic...\n",
       "3                7 on 9/27/75 Audit C Score Current:\\n\n",
       "4    2/6/96 sleep studyPain Treatment Pain Level (N...\n",
       "5                    .Per 7/06/79 Movement D/O note:\\n\n",
       "6    4, 5/18/78 Patient's thoughts about current su...\n",
       "7    10/24/89 CPT Code: 90801 - Psychiatric Diagnos...\n",
       "8                         3/7/86 SOS-10 Total Score:\\n\n",
       "9             (4/10/71)Score-1Audit C Score Current:\\n\n",
       "dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "doc = []\n",
    "with open('dates.txt') as file:\n",
    "    for line in file:\n",
    "        doc.append(line)\n",
    "\n",
    "df = pd.Series(doc)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    474\n",
       "1    153\n",
       "2     13\n",
       "3    129\n",
       "4     98\n",
       "5    111\n",
       "6    225\n",
       "7     31\n",
       "8    171\n",
       "9    191\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def date_sorter():\n",
    "    # Step 1: Regex\n",
    "    ## A) Numeric date. E.g., mm/dd/yyyy, mm/dd/yy, mm-dd-yyyy, mm-dd-yyy\n",
    "    num = '(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})'\n",
    "    ## B) Date. E.g., Month Day Year, Jan 4th 2000, August 2nd, 1999, Oct. 1, 1987\n",
    "    day_second = '((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[.]?[a-z]*[+\\s]\\d{1,2}(?:st|nd|rd|th)?[,]?[+\\s]\\d{4})'\n",
    "    ## C) Date, with day optional. E.g., Day Month Year, 14 Feb. 2009, 22 July 2001, Mar 2009\n",
    "    day_first = '((\\d{1,2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[.]?[a-z]*[,]? \\d{4})'\n",
    "    ## D) Obtain year (yyyy  year format) and optionally the month (m or mm  month format) for the 1900s and 2000s \n",
    "    year = '((\\d{1,2}[/-])?[1|2]\\d{3})'\n",
    "\n",
    "    ## Compile a list of all extracted dates\n",
    "    all_dates = '({}|{}|{}|{})'.format(num, day_first, day_second, year)\n",
    "    date = df.str.extract(all_dates, expand=True)\n",
    "\n",
    "    # Step 2. Correct mispellings \n",
    "    date = date[date.columns[0]].str.replace('Janaury', 'January').str.replace('Decemeber', 'December')\n",
    "\n",
    "    # Step 3: Final result. Convert series to datetime in ascending order\n",
    "    date = pd.to_datetime(date)\n",
    "    return pd.Series(date.sort_values().index)\n",
    "\n",
    "date_sorter().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Assignment 2 - Introduction to NLTK\n",
    "\n",
    "In part 1 of this assignment, I used nltk to explore the Herman Melville novel, *Moby Dick*. Then in part 2, I created a spelling recommender function that uses nltk to find words similar to the misspelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Analyzing Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "# If you would like to work with the raw text you can use 'moby_raw'\n",
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "    \n",
    "# If you would like to work with the novel in nltk.Text format you can use 'text1'\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "text1 = nltk.Text(moby_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 - How many tokens (words and punctuation symbols) are in text1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254989"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_one():\n",
    "    \n",
    "    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)\n",
    "\n",
    "example_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 - How many unique tokens (unique words and punctuation) does text1 have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20755"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_two():\n",
    "    \n",
    "    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))\n",
    "\n",
    "example_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 - After lemmatizing the verbs, how many unique tokens does text1 have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16900"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def example_three():\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
    "\n",
    "    return len(set(lemmatized))\n",
    "\n",
    "example_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08139566804842562"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_one():\n",
    "\n",
    "    # Ratio =  (Unique Tokens) / (Total # of Tokens)\n",
    "    return  len(set(moby_tokens)) / len(moby_tokens)\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - What percentage of tokens is 'whale' or 'Whale'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4125668166077752"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "    \n",
    "    # Step 1) Find number of \"whale\" and \"Whale\" tokens\n",
    "    whale = text1.count(\"whale\") + text1.count(\"Whale\")\n",
    "    \n",
    "    # Step 2) Find percentage: (Whale Tokens) / (Total Tokens) * 100%\n",
    "    return (whale / len(moby_tokens)) * 100\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7308),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978),\n",
       " ('his', 2459),\n",
       " ('it', 2196),\n",
       " ('I', 2097),\n",
       " ('!', 1767),\n",
       " ('is', 1722),\n",
       " ('--', 1713),\n",
       " ('with', 1659),\n",
       " ('he', 1658),\n",
       " ('was', 1639),\n",
       " ('as', 1620)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "dist = FreqDist(text1)\n",
    "\n",
    "def answer_three():\n",
    "    return dist.most_common(20)\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 - What tokens have a length of greater than 5 and frequency of more than 150?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain',\n",
       " 'Pequod',\n",
       " 'Queequeg',\n",
       " 'Starbuck',\n",
       " 'almost',\n",
       " 'before',\n",
       " 'himself',\n",
       " 'little',\n",
       " 'seemed',\n",
       " 'should',\n",
       " 'though',\n",
       " 'through',\n",
       " 'whales',\n",
       " 'without']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_four():\n",
    "    freq_words = [t for t in text1 if len(t) > 5 and dist[t] > 150]\n",
    "    # Note: Use set() to obtain unique tokens\n",
    "    return sorted(set(freq_words))\n",
    "\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 - Find the longest word in text1 and that word's length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"twelve-o'clock-at-night\", 23)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique words within the novel:\n",
    "words = set(text1)\n",
    "    \n",
    "def answer_five():\n",
    "    \n",
    "    #Order words by decreasing length\n",
    "    longest_word = sorted(dist, reverse=True, key=len) \n",
    "    \n",
    "    return (longest_word[0], len(longest_word[0]))\n",
    "\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 - What unique words have a frequency of more than 2000? What is their frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13715, 'the'),\n",
       " (6513, 'of'),\n",
       " (6010, 'and'),\n",
       " (4545, 'a'),\n",
       " (4515, 'to'),\n",
       " (3908, 'in'),\n",
       " (2978, 'that'),\n",
       " (2459, 'his'),\n",
       " (2196, 'it'),\n",
       " (2097, 'I')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():\n",
    "   \n",
    "    # Determine unique words and their frequencies for words occuring more than 2000 times: \n",
    "    filtered = dict((freq, word) for word, freq in dist.items() if word.isalpha() and dist[word] > 2000)\n",
    "    \n",
    "    # Return list of unique words, ordered from most to least frequent\n",
    "    return sorted(filtered.items(), reverse=True)\n",
    "\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 - What is the average number of tokens per sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.881952902963864"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_seven():\n",
    "    # Tokenize words \n",
    "    sent_tokens = nltk.sent_tokenize(moby_raw)\n",
    "    # Number of tokens per sentence\n",
    "    num_sentence = [len(nltk.word_tokenize(w)) for w in sent_tokens]\n",
    "    # Average length of words per sentence\n",
    "    return np.mean(num_sentence)\n",
    "\n",
    "answer_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 - What are the 5 most frequent parts of speech in this text? What is their frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "The top 5 most frequent parts of speech: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('NN', 32730), ('IN', 28657), ('DT', 25867), (',', 19204), ('JJ', 17620)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "from collections import Counter \n",
    "\n",
    "def answer_eight():\n",
    "    # Parts of speech for the entire novel:\n",
    "    pos_tag = nltk.pos_tag(text1)\n",
    "    # Count the most frequent parts of speech (pos):\n",
    "    freq_pos = Counter(part for word, part in pos_tag)\n",
    "    \n",
    "    print(\"The top 5 most frequent parts of speech: \")\n",
    "    return freq_pos.most_common(5)\n",
    "\n",
    "answer_eight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Spelling Recommender\n",
    "\n",
    "For this part of the assignment, I created three different spelling recommenders. Each take a list of misspelled words and recommends a correctly spelled word for every word in the list.\n",
    "\n",
    "For every misspelled word, the recommender finds the word in `correct_spellings` that has the shortest distance, and starts with the same letter as the misspelled word, and return that word as a recommendation. This distance is calculated using either the Jaccard Distance or Edit Distance. \n",
    "\n",
    "Each of the recommenders provides recommendations for the three default words provided: `['cormulent', 'incendenece', 'validrate']`.\n",
    "\n",
    "<br>\n",
    "*Distance metrics*:\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index)** and **[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)**\n",
    "\n",
    "\n",
    "*Note: Since there is a lot of overlapping code for the three recommendation systems, I created a generalized recommendation function that analyzes the word entries based on the number of specified ngrams. The function also includes an option for using the Jaccard or Edit distance, as specified in Questions 9-11.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'Aani',\n",
       " 'aardvark',\n",
       " 'aardwolf',\n",
       " 'Aaron']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "correct_spellings = words.words()\n",
    "correct_spellings[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 - For this recommender, provide recommendations for the three default words using the Jaccard Distance on the trigrams of the two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpulent', 'indecence', 'validate']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this function for Question #9-11:\n",
    "\n",
    "def recommend(entries, ngram):\n",
    "    \n",
    "    distance = []\n",
    "\n",
    "    for e in entries:\n",
    "        correct = [cor for cor in correct_spellings if cor[0].startswith(e[0])]\n",
    "        \n",
    "        # Optional: Edit Distance (Question #11)\n",
    "        if ngram == 0:\n",
    "            edit = [nltk.edit_distance(e, cor) for cor in correct]\n",
    "            distance.append(correct[np.argmin(edit)])\n",
    "            \n",
    "        # Default: Jaacard's Distance (Question #9-10)\n",
    "        else: \n",
    "            jaccard = [nltk.jaccard_distance(set(nltk.ngrams(e, n=ngram)), set(nltk.ngrams(cor, n=ngram)))\n",
    "                                            for cor in correct]\n",
    "            distance.append(correct[np.argmin(jaccard)])\n",
    "    \n",
    "    return distance\n",
    "    # Return the closest recomendation for misspelled word\n",
    "\n",
    "def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    \n",
    "    return recommend(entries, ngram=3)\n",
    "    \n",
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 - For this recommender, provide recommendations for the three default words using the Jaccard Distance on the 4-grams of the two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cormus', 'incendiary', 'valid']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    \n",
    "    return recommend(entries, ngram=4)\n",
    "    \n",
    "answer_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11 - For this recommender, provide recommendations for the three default words using the Edit Distance of the two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpulent', 'intendence', 'validate']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    \n",
    "    # Note: Use ngram=0 to measure Edit Distance rather than the default Jacard's Distance\n",
    "    return recommend(entries, ngram=0)\n",
    "    \n",
    "answer_eleven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Assignment 3 - Text Classifications\n",
    "\n",
    "In this assignment, I explored text message data and created models to predict if a message is spam or not. These models included Logistic Regression, Naive Bayes, and anSupport Vector Machine. I accounted for text features, such as length of document, along with the number and type of characters included within a text message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - What percentage of the documents in `spam_data` are spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_one():\n",
    "    ratio = spam_data['target'].mean()\n",
    "    return ratio * 100\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the training data `X_train` using a Count Vectorizer with default parameters.**\n",
    "\n",
    "### Question 2 - What is the longest token in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'com1win150ppmx3age16subscription'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def answer_two():\n",
    "    \n",
    "    ## A) Count Vectorizer: Convert training text into counts per word, a.k.a., \"tokens\"\n",
    "    count_vect = CountVectorizer().fit(X_train)\n",
    "    \n",
    "    ## B) Find longest token\n",
    "    tokens = count_vect.get_feature_names()\n",
    "    longest = sorted(tokens, reverse=True, key=len)\n",
    "    \n",
    "    return longest[0]\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit and transform the training data `X_train` using a Count Vectorizer with default parameters.** \n",
    "\n",
    "**Next, fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1`.**\n",
    "\n",
    "### Question 3 - What is the area under the curve (AUC) score using the transformed test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97208121827411165"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def answer_three():\n",
    "    \n",
    "    # Step 1) Count Vectorizer: Fit and Transform text into matrix-form counts per word, a.k.a., \"tokens\"\n",
    "    count_vect = CountVectorizer().fit(X_train)\n",
    "    X_train_vect = count_vect.transform(X_train)\n",
    "        \n",
    "    # Step 2) Multinomial Naive Bayes:\n",
    "    model = MultinomialNB(alpha=0.1)\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    \n",
    "    # Step 3) Predict spam/not-spam. Obtain AUC Score by comparing predictions to test set\n",
    "    predict = model.predict(count_vect.transform(X_test))\n",
    "    auc_score = roc_auc_score(y_test, predict)\n",
    "    \n",
    "    return auc_score\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit and transform the training data `X_train` using a Tfidf Vectorizer with default parameters.**\n",
    "\n",
    "### Question 4 - What 20 features have the smallest tf-idf and what 20 have the largest tf-idf?\n",
    "\n",
    "**Put these features in a two series where each series is sorted by tf-idf value and then alphabetically by feature name. The index of the series should be the feature name, and the data should be the tf-idf.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sympathetic     0.074475\n",
       " healer          0.074475\n",
       " aaniye          0.074475\n",
       " dependable      0.074475\n",
       " companion       0.074475\n",
       " listener        0.074475\n",
       " athletic        0.074475\n",
       " exterminator    0.074475\n",
       " psychiatrist    0.074475\n",
       " pest            0.074475\n",
       " determined      0.074475\n",
       " chef            0.074475\n",
       " courageous      0.074475\n",
       " stylist         0.074475\n",
       " psychologist    0.074475\n",
       " organizer       0.074475\n",
       " pudunga         0.074475\n",
       " venaam          0.074475\n",
       " diwali          0.091250\n",
       " mornings        0.091250\n",
       " dtype: float64, 146tf150p    1.000000\n",
       " havent       1.000000\n",
       " home         1.000000\n",
       " okie         1.000000\n",
       " thanx        1.000000\n",
       " er           1.000000\n",
       " anything     1.000000\n",
       " lei          1.000000\n",
       " nite         1.000000\n",
       " yup          1.000000\n",
       " thank        1.000000\n",
       " ok           1.000000\n",
       " where        1.000000\n",
       " beerage      1.000000\n",
       " anytime      1.000000\n",
       " too          1.000000\n",
       " done         1.000000\n",
       " 645          1.000000\n",
       " tick         0.980166\n",
       " blank        0.932702\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use for Questions 4:\n",
    "## Takes in vectorizer, training data, and n-grams\n",
    "def coeff(vect, train, n, added):\n",
    "    \n",
    "    # A) Find features:\n",
    "    features = np.array(vect.get_feature_names() + added)\n",
    "    \n",
    "    # B) Sorted features and indices:\n",
    "    maxed = train.max(0).toarray()[0]\n",
    "    sorted_index = maxed.argsort()\n",
    "    sorted_features = maxed[sorted_index]\n",
    "    \n",
    "    # C) Find n-smallest and n-largest coefficients:\n",
    "    small_coeff = pd.Series(sorted_features[:n], index=features[sorted_index[:n]])\n",
    "    large_coeff = pd.Series(sorted_features[-n:][::-1], index=features[sorted_index[-n:][::-1]])\n",
    "    \n",
    "    return small_coeff, large_coeff\n",
    "\n",
    "\n",
    "# Find the 20 smallest and largest coefficients with Tfidf Vectorizer\n",
    "def answer_four():\n",
    "    \n",
    "    tf_vect = TfidfVectorizer().fit(X_train)\n",
    "    X_train_vect = tf_vect.transform(X_train)\n",
    "    \n",
    "    smallest, largest = coeff(tf_vect, X_train_vect, n=20, added=[])\n",
    "    \n",
    "    return (smallest, largest)\n",
    "\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **3**.**\n",
    "\n",
    "**Then fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1`**\n",
    "\n",
    "### Question 5 - After transforming the test data (described above), compute the area under the curve (AUC) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94162436548223349"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_five():\n",
    "    \n",
    "    # Step 1) TFIDF: Fit and Transform text \n",
    "    vect = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "    X_train_vect = vect.transform(X_train)\n",
    "    \n",
    "    # Step 2) Multinomial Naive Bayes:\n",
    "    model = MultinomialNB(alpha=0.1)\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    \n",
    "    # Step 3) Predict spam/not-spam. Obtain AUC Score by comparing predictions to test set\n",
    "    predict = model.predict(vect.transform(X_test))\n",
    "    auc_score = roc_auc_score(y_test, predict)\n",
    "    \n",
    "    return auc_score\n",
    "\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 - What is the average length of documents (number of characters) for not spam and spam documents?\n",
    "\n",
    "*Note: Since many questions involve finding the number of characters, I created a 'num_char' function to avoid repeated code.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71.023626943005183, 138.8661311914324)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use for Questions 6-7, 9 and 11:\n",
    "## Number of characters/Length of document (e.g., for non-spam and spam text, training and testing data)\n",
    "def num_char(a, b):\n",
    "    char_a = [len(w) for w in a]\n",
    "    char_b = [len(w) for w in b]\n",
    "    return char_a, char_b\n",
    "\n",
    "def answer_six():\n",
    "    \n",
    "    # Use to find characters for non-spam and spam:\n",
    "    target = spam_data['target']\n",
    "    not_spam = spam_data.loc[target==0, 'text']\n",
    "    spam = spam_data.loc[target==1, 'text']\n",
    "    \n",
    "    # Number of characters:\n",
    "    not_spam, spam = num_char(not_spam, spam)\n",
    "    \n",
    "    return (np.average(not_spam), np.average(spam))\n",
    "\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "The following function has been provided to help combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit and transform the training data X_train using a Tfidf Vectorizer, ignoring terms that have a document frequency strictly lower than **5**.**\n",
    "\n",
    "**Using this document-term matrix and an additional feature, **the length of document (number of characters)**, fit a Support Vector Classification model with regularization `C=10000`.**\n",
    "\n",
    "### Question 7 - After transforming the test data (described above), compute the area under the curve (AUC) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95813668234215565"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def answer_seven():\n",
    "    \n",
    "    # Step 1) Fit and tranform data\n",
    "    vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "    X_train_vect = vect.transform(X_train)\n",
    "    ## Use transformed y_test later for model predictions:\n",
    "    X_test_vect = vect.transform(X_test)\n",
    "    \n",
    "    # Step 2) Add feature to model: Number of characters (a.k.a. length of document)\n",
    "    ## Use X_train for model fitting and later X_test later for model predictions\n",
    "    char_train, char_test = num_char(X_train, X_test)\n",
    "\n",
    "    ## Added feature: number of characters in documment\n",
    "    X_train_added = add_feature(X_train_vect, char_train)\n",
    "    X_test_added = add_feature(X_test_vect, char_test)\n",
    "    \n",
    "    # Step 3) Fit model: Support Vector Classification\n",
    "    model = SVC(C=10000).fit(X_train_added, y_train)\n",
    "    \n",
    "    # Step 4) Model predictions & Evaluation: AUC score\n",
    "    predict = model.predict(X_test_added)\n",
    "    auc_score = roc_auc_score(y_test, predict)\n",
    "    \n",
    "    return auc_score\n",
    "\n",
    "answer_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 - What is the average number of digits per document for not spam and spam documents?\n",
    "\n",
    "*Note: Since many questions involve finding the number of digits, I created a 'num_digits' function to avoid repeated code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29927461139896372, 15.76037483266399)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use function for questions 8-9, and 11:\n",
    "## Number of digits (e.g., for not-spam and spam documents, training and testing data):\n",
    "def num_digits(a, b):\n",
    "    num_a = [sum(x.isnumeric() for x in d) for d in a]\n",
    "    num_b = [sum(x.isnumeric() for x in d) for d in b]\n",
    "    \n",
    "    return num_a, num_b\n",
    "\n",
    "\n",
    "def answer_eight():\n",
    "    # Use to find digits for non-spam and spam text:\n",
    "    target = spam_data['target']\n",
    "    not_spam = spam_data.loc[target==0, 'text']\n",
    "    spam = spam_data.loc[target==1, 'text']\n",
    "    \n",
    "    # Number of digits:\n",
    "    not_spam, spam = num_digits(not_spam, spam)\n",
    "    \n",
    "    return (np.average(not_spam), np.average(spam))\n",
    "\n",
    "answer_eight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **word n-grams from n=1 to n=3** (unigrams, bigrams, and trigrams).**\n",
    "\n",
    "**Using this document-term matrix, fit a Logistic Regression model with regularization `C=100`, using the following additional features:**\n",
    "* the length of document (number of characters)\n",
    "* **number of digits per document**\n",
    "\n",
    "### Question 9 - After transforming the test data (described above), compute the area under the curve (AUC) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96533283533945646"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def answer_nine():\n",
    "    \n",
    "    # Step 1) TFidf Vectorizer\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)\n",
    "    X_train_vect = vect.transform(X_train)\n",
    "    X_test_vect = vect.transform(X_test)\n",
    "    \n",
    "    # Step 2) Update model, upon finding the number of...\n",
    "    ## Characters (length of document):\n",
    "    char_train, char_test = num_char(X_train, X_test)\n",
    "    ## Digits per document:\n",
    "    num_train, num_test = num_digits(X_train, X_test)\n",
    "    \n",
    "    ## Update model with added features\n",
    "    X_train_added = add_feature(X_train_vect, [char_train, num_train])\n",
    "    X_test_added = add_feature(X_test_vect, [char_test, num_test])\n",
    "    \n",
    "    # Step 3) Logistic regression model\n",
    "    model = LogisticRegression(C=100).fit(X_train_added, y_train)\n",
    "    \n",
    "    # Step 4) Evaluation: AUC score\n",
    "    predict = model.predict(X_test_added)\n",
    "    auc_score = roc_auc_score(y_test, predict)\n",
    "    \n",
    "    return auc_score\n",
    "\n",
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 - What is the average number of non-word characters (anything other than a letter, digit or underscore) per document for not spam and spam documents?\n",
    "\n",
    "*Note: Since many questions involve finding the number of non-word characters, I created a 'num_non_words' function to avoid repeated code.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use for Question 10-11:\n",
    "## Number of non_word characers (E.g., for non-spam and spam documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.291813471502589, 29.041499330655956)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_non_words(a, b):\n",
    "    char_a = a.str.count('\\W')\n",
    "    char_b = b.str.count('\\W')\n",
    "    \n",
    "    return char_a, char_b\n",
    "\n",
    "def answer_ten():\n",
    "    \n",
    "    # Use to find non-word characters for non-spam and spam text:\n",
    "    target = spam_data['target']\n",
    "    not_spam = spam_data.loc[target==0, 'text']\n",
    "    spam = spam_data.loc[target==1, 'text']\n",
    "    \n",
    "    # Number of non-word characters:\n",
    "    not_spam, spam = num_non_words(not_spam, spam)\n",
    "    \n",
    "    return (np.average(not_spam), np.average(spam))\n",
    "    \n",
    "answer_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the finale...\n",
    "\n",
    "**Fit and transform the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.****\n",
    "\n",
    "To tell Count Vectorizer to use character n-grams, **pass in `analyzer='char_wb'`.** This creates character n-grams only from text inside word boundaries. This will also make the model more robust to spelling mistakes.\n",
    "\n",
    "**Using this document-term matrix, fit a Logistic Regression model with regularization `C=100`, using the following additional features:**\n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "* **number of non-word characters (anything other than a letter, digit or underscore.)**\n",
    "\n",
    "\n",
    "### Question 11 - After transforming the test data (described above), compute the area under the curve (AUC) score.  Also, **find the 10 smallest and 10 largest coefficients from the model** and return them along with the AUC score in a tuple.\n",
    "\n",
    "*Note: The list of 10 smallest coefficients should be sorted smallest first, the list of 10 largest coefficients should be sorted largest first.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.97885931107074342,\n",
       " ['. ', '..', '? ', ' i', ' y', ' go', ':)', ' h', 'go', ' m'],\n",
       " ['digit_count', 'ne', 'ia', 'co', 'xt', ' ch', 'mob', ' x', 'ww', 'ar'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resembles Question 9, but combines several exercises together...\n",
    "\n",
    "def answer_eleven():\n",
    "    \n",
    "    # Step 1) Count Vectorizer: Find matrix-form counts per word, a.k.a., \"tokens\"\n",
    "    ## Use character n-grams from test inside word boundaries ('char_wb') \n",
    "    count_vect = CountVectorizer(min_df=5, ngram_range=(2,5), analyzer='char_wb').fit(X_train)\n",
    "    X_train_vect = count_vect.transform(X_train)\n",
    "    ## Use for model evaluation:\n",
    "    X_test_vect = count_vect.transform(X_test)\n",
    "    \n",
    "    \n",
    "    # Step 2) Find the number of...\n",
    "    ## Characters (length of document):\n",
    "    char_train, char_test = num_char(X_train, X_test)\n",
    "    ## Digits per document:\n",
    "    num_train, num_test = num_digits(X_train, X_test)\n",
    "    ## Non-word characters:\n",
    "    non_train, non_test = num_non_words(X_train, X_test)\n",
    "    \n",
    "    ## Update document-matrix with added training and testing features\n",
    "    X_train_added = add_feature(X_train_vect, [char_train, num_train, non_train])\n",
    "    X_test_added = add_feature(X_test_vect, [char_test, num_test, non_test])\n",
    "    \n",
    "    \n",
    "    # Step 3) Logistic regression model\n",
    "    model = LogisticRegression(C=100).fit(X_train_added, y_train)\n",
    "    \n",
    "    # Step 4) Evaluation: AUC score\n",
    "    predict = model.predict(X_test_added)\n",
    "    auc_score = roc_auc_score(y_test, predict)\n",
    "    \n",
    "    # Step 5) Find 10 smallest and largest coefficients:\n",
    "    features = np.array(count_vect.get_feature_names() + ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "    sorted_index = model.coef_[0].argsort()\n",
    "    smallest = list(features[sorted_index[:10]])\n",
    "    largest = list(features[sorted_index[:-11:-1]])\n",
    "    \n",
    "    return (auc_score, smallest, largest)\n",
    "\n",
    "answer_eleven()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "LvcWI",
   "launcher_item_id": "krne9",
   "part_id": "Mkp1I"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
